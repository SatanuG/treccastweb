## Baseline Runs

There are several baselines across the Main and Mixed Iniitative tasks for the Year 4 benchmark. 

The Main Task baseline system uses a similar architecture as [Year 3](2021/baselines/Readme.md) (i.e BM25  document retrieval + T5 passage reranking) but with a different index and a [BART summariser](https://huggingface.co/facebook/bart-large-cnn) for generating a summary of the top three passages from the retrieval stage.

- `BM25_T5_BART_automatic.json` is the run generated by the baseline system using each turn's automatic rewrite
- `BM25_T5_BART_manual.json` is the run generated by the baseline system using each turn's manual rewrite

The Mixed Initiative task baselines are split into question generation and question ranking runs. 

### Generative

The generative runs use GPT3 and T5 with different types of input to generate questions. The T5 model was finetuned on the [Clariq dataset](https://github.com/aliannejadi/ClariQ).

- [`GPT-3_full_context_run.json`](2022/baselines/mi_task/generative/GPT-3_full_context_run.json) uses the full conversation history (user utterances and system responses) as input to GPT3
- [`GPT-3_raw_run.json`](2022/baselines/mi_task/generative/GPT-3_raw_run.json) uses the raw user utterance at each turn as input to GPT3
- [`GPT-3_rewrite_run.json`](2022/baselines/mi_task/generative/GPT-3_rewrite_run.json) uses the automatic rewrite as input to GPT3

- [`T5_raw_run.json`](2022/baselines/mi_task/generative/T5_raw_run.json) uses the raw user utterance as input to T5
- [`T5_rewrite_run.json`](2022/baselines/mi_task/generative/T5_rewrite_run.json) uses the automatic rewrite as input to T5

### Ranking

The ranking runs rank questions from the [question pool](2022/2022_mixed_initiative_question_pool.json).

- [`bm25_baseline_mi_run.json`](2022/baselines/mi_task/ranking/bm25_baseline_mi_run.json) ranks questions using BM25 with the automatic rewrite of the current turn query

- [`miniLM_bert_sample_mi_run.json`](2022/baselines/mi_task/ranking/miniLM_bert_sample_mi_run.json) generates a candidate pool of questions using the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) from Sentence Transformers, then reranks them using a BERT model trained on the Clariq dataset for pointwise question classification.